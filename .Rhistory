source('~/.active-rstudio-document')
getwd()
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
xgb.sur<-function(datax=x_train,datay=y,method=c('defaut','pl','C'),nfolds=5,nround=NULL,
lambda=NULL,alpha=NULL,eta=NULL,early_stopping_rounds=NULL
)
{
method=match.arg(method)
if(is.null(lambda))
lambda=.01
if(is.null(alpha))
alpha=.01
if(is.null(eta))
eta=.01
if(is.null(nround))
nround=1000
if(is.null(early_stopping_rounds))
early_stopping_rounds=20
tt<-length(x_train[,1])
surv_time <- Surv(y$time,y$status)
y_train_boost <-  2 * y[,2] * (y[,1] - .5) #make fisrt col status and second col time
y_train<-surv_time
XDtrain <- xgb.DMatrix(x_train, label = y_train_boost)
if(method=='C')
model<-xgb.cv(list(objective = cidx_xgb_obj, eval_metric = cidx_xgb_func,
tree_method = 'hist', grow_policy = 'lossguide',
eta = eta, lambda = lambda, alpha = alpha, subsample = .5,
colsample_bytree = .5), XDtrain, nround = nround,
nfold = nfolds, verbose = F, early_stopping_rounds = early_stopping_rounds, maximize = T,
callbacks = list(cb.cv.predict(T)))
else    model<-xgb.cv(list(objective = 'survival:cox', eval_metric = cidx_xgb_func,
tree_method = 'hist', grow_policy = 'lossguide',
eta = eta, lambda = lambda, alpha = alpha, subsample = .5,
colsample_bytree = .5), XDtrain, nround = nround,
nfold = nfolds, verbose = F, early_stopping_rounds = early_stopping_rounds, maximize = T,
callbacks = list(cb.cv.predict(T)))
model
}
n <- 1000
p <- 100
rho <- 0.5
tt <- round(.8*n)
V <- diag(p)
X <- mvrnorm(n = n, mu = rep(0, p), Sigma = V)
colnames(X)<-xname
### nonlinear transformation
mu <- exp(2*pnorm((X[, 10] > 0.5) + X[, 20] ^ 2 - 1) +
2*pnorm(0.5*X[, 30]+X[, 40]^2 - 1) +
2*pnorm(0.5 * X[, 50]+X[, 60] ^ 2 - 1) +
2*pnorm(sin(X[, 70]) + X[, 80] ^ 2 - 1) +
2*pnorm(cos(X[, 90])+X[, 100] ^ 2 - 1))
### survival time simulation
obs_time <- -(log(runif(n)))/(mu)
a <- 2*rbinom(n = n, size = 1, prob = 1/3)
b <- runif(n = n, min = 0, max = 2)
a[a == 0] <- b[a == 0]
C <- a
obs_time <- pmin(obs_time, C)
status <- as.numeric(obs_time <= C)
surv_time <- Surv(obs_time, status)
surv_time_boost <-  2 * obs_time * (status - .5)
x_train <- X[seq_len(tt), ]
y_train <- surv_time[seq_len(tt)]
y_train_boost <- surv_time_boost[seq_len(tt)]
x_test <- X[(tt + 1) : n, ]
y_test <- surv_time[(tt + 1) : n]
y_test_boost <- surv_time_boost[(tt + 1) : n]
y<-cbind(status,obs_time)
View(y)
colnames(y)<-c('status','time')
View(y)
source('~/.active-rstudio-document')
m<-xgb.sur(datax=x_train,datay = y,method = 'pl')
y$time
y<-as.data.frame(y)
y$time
m<-xgb.sur(datax=x_train,datay = y,method = 'pl')
y_train_boost <-  2 * y[,2] * (y[,1] - .5)
XDtrain <- xgb.DMatrix(x_train, label = y_train_boost)
length(y_train_boost)
length(x_train[,1])
XDtrain <- xgb.DMatrix(X, label = y_train_boost)
m<-xgb.sur(datax=X,datay = y,method = 'pl')
source('~/Downloads/xgblgbcox/R/loss_func.R')
m<-xgb.sur(datax=X,datay = y,method = 'pl')
source('loss_func.R')
getwd()
source('loss_func.R')
source('~/Downloads/xgblgbcox/R/loss_func.R')
getwd()
m<-xgb.sur(datax=X,datay = y)
library(MASS)
library(lightgbm)
library(survival)
library(dplyr)
library(magrittr)
library(survcomp)
library(data.table)
source('loss_func.R')
#using xgboost with loss function of cox paritial likelihood or cindex
lgb.sur<-function(datax,datay,method=c('defaut','pl','C'),nfolds=5,nround=NULL,
lambda=NULL,alpha=NULL,eta=NULL,early_stopping_rounds=NULL
)
{
x_train=datax
y=datay
method=match.arg(method)
if(is.null(lambda))
lambda=.01
if(is.null(alpha))
alpha=.01
if(is.null(eta))
eta=.01
if(is.null(nround))
nround=1000
if(is.null(early_stopping_rounds))
early_stopping_rounds=20
tt<-length(x_train[,1])
surv_time <- Surv(y$time,y$status)
y_train_boost <-  2 * y[,2] * (y[,1] - .5) #make fisrt col status and second col time
#y_train<-surv_time
LDtrain <- lgb.Dataset(x_train, label = y_train_boost)
if(method=='C')
model<-lgb.cv(list(objective = cidx_xgb_obj,
eta = eta, lambda = lambda, alpha = alpha, subsample = .5,
colsample_bytree = .5), LDtrain, nround = nround,eval=cidx_lgb_func,
nfold = nfolds, verbose = 0, early_stopping_rounds = early_stopping_rounds)
else    model<-lgb.cv(list(objective = Cox_lgb_obj,
eta = eta, lambda = lambda, alpha = alpha, subsample = .5,
colsample_bytree = .5), LDtrain, nround = nround,eval=cidx_lgb_func,
nfold = nfolds, verbose = 0, early_stopping_rounds = early_stopping_rounds
model
}
source('~/.active-rstudio-document')
source('~/Downloads/xgblgbcox/R/lgb.sur.R')
lgb.sur<-function(datax,datay,method=c('defaut','pl','C'),nfolds=5,nround=NULL,
lambda=NULL,alpha=NULL,eta=NULL,early_stopping_rounds=NULL
)
{
x_train=datax
y=datay
method=match.arg(method)
if(is.null(lambda))
lambda=.01
if(is.null(alpha))
alpha=.01
if(is.null(eta))
eta=.01
if(is.null(nround))
nround=1000
if(is.null(early_stopping_rounds))
early_stopping_rounds=20
tt<-length(x_train[,1])
surv_time <- Surv(y$time,y$status)
y_train_boost <-  2 * y[,2] * (y[,1] - .5) #make fisrt col status and second col time
#y_train<-surv_time
LDtrain <- lgb.Dataset(x_train, label = y_train_boost)
if(method=='C')
model<-lgb.cv(list(objective = cidx_xgb_obj,
eta = eta, lambda = lambda, alpha = alpha, subsample = .5,
colsample_bytree = .5), LDtrain, nround = nround,eval=cidx_lgb_func,
nfold = nfolds, verbose = 0, early_stopping_rounds = early_stopping_rounds)
else    model<-lgb.cv(list(objective = Cox_lgb_obj,
eta = eta, lambda = lambda, alpha = alpha, subsample = .5,
colsample_bytree = .5), LDtrain, nround = nround,eval=cidx_lgb_func,
nfold = nfolds, verbose = 0, early_stopping_rounds = early_stopping_rounds)
model
}
m<-lgb.sur(datax=X,datay = y)
n <- 1000
p <- 100
rho <- 0.5
tt <- round(.8*n)
V <- diag(p)
X <- mvrnorm(n = n, mu = rep(0, p), Sigma = V)
colnames(X)<-xname
### nonlinear transformation
mu <- exp(2*pnorm((X[, 10] > 0.5) + X[, 20] ^ 2 - 1) +
2*pnorm(0.5*X[, 30]+X[, 40]^2 - 1) +
2*pnorm(0.5 * X[, 50]+X[, 60] ^ 2 - 1) +
2*pnorm(sin(X[, 70]) + X[, 80] ^ 2 - 1) +
2*pnorm(cos(X[, 90])+X[, 100] ^ 2 - 1))
### survival time simulation
obs_time <- -(log(runif(n)))/(mu)
a <- 2*rbinom(n = n, size = 1, prob = 1/3)
b <- runif(n = n, min = 0, max = 2)
a[a == 0] <- b[a == 0]
C <- a
obs_time <- pmin(obs_time, C)
status <- as.numeric(obs_time <= C)
y<-cbind(status,obs_time)
colnames(y)<-c('status','time')
y<-as.data.frame(y)
surv_time_boost <-  2 * obs_time * (status - .5)
x_train <- X[seq_len(tt), ]
y_train <- y[seq_len(tt),]
x_test <- X[(tt + 1) : n, ]
y_test <- y[(tt + 1) : n,]
y_test_boost <- surv_time_boost[(tt + 1) : n]
xgb_cox_m<-xgb.sur(x_train,y_train)
xgb_cix_m<-xgb.sur(x_train,y_train,method = 'C')
lgb_cox_m<-lgb.sur(x_train,y_train)
lgb_cix_m<-lgb.sur(x_train,y_train,method = 'C')
### Convert test data to XGB/LGB dataset
XDtest <- xgb.DMatrix(x_test, label = y_test_boost)
LDtest <- lgb.Dataset(x_test, label = y_test_boost)
#prediction
#xgb model
y_xgcox_predict <- -rowMeans(sapply(xgb_cox_m$models, predict, XDtest))
y_xgcidx_predict <- -rowMeans(sapply(xgb_cix_m$models, predict, XDtest))
#lgb model
y_lgcox_pred <- y_lgcidx_pred <- matrix(0, n - tt, 5)
for (i in seq_len(5)) {
y_lgcox_pred[, i] <- predict(lgb_cox_m$boosters[[i]]$booster, x_test)
y_lgcidx_pred[, i] <- predict(lgb_cix_m$boosters[[i]]$booster, x_test)
}
y_lgcox_predict <- -rowMeans(y_lgcox_pred)
y_lgcidx_predict <- -rowMeans(y_lgcidx_pred)
#validation cindex value
cidx_result <- c('XGB_Cox' = concordance(y_test ~ y_xgcox_predict)$con,
'XGB_Cidx' = concordance(y_test ~ y_xgcidx_predict)$con,
'LGB_Cox' = concordance(y_test ~ y_lgcox_predict)$con,
'LGB_Cidx' = concordance(y_test ~ y_lgcidx_predict)$con)
print(cidx_result)
lgb.sur<-function(datax,datay,method=c('defaut','pl','C'),nfolds=5,nround=NULL,
lambda=NULL,alpha=NULL,eta=NULL,early_stopping_rounds=NULL
)
{
x_train=datax
y=datay
method=match.arg(method)
if(is.null(lambda))
lambda=.01
if(is.null(alpha))
alpha=.01
if(is.null(eta))
eta=.01
if(is.null(nround))
nround=1000
if(is.null(early_stopping_rounds))
early_stopping_rounds=20
tt<-length(x_train[,1])
surv_time <- Surv(y$time,y$status)
y_train_boost <-  2 * y[,2] * (y[,1] - .5) #make fisrt col status and second col time
#y_train<-surv_time
LDtrain <- lgb.Dataset(x_train, label = y_train_boost)
if(method=='C')
model<-lgb.cv(list(objective = cidx_lgb_obj,
eta = eta, lambda = lambda, alpha = alpha, subsample = .5,
colsample_bytree = .5), LDtrain, nround = nround,eval=cidx_lgb_func,
nfold = nfolds, verbose = 0, early_stopping_rounds = early_stopping_rounds)
else    model<-lgb.cv(list(objective = Cox_lgb_obj,
eta = eta, lambda = lambda, alpha = alpha, subsample = .5,
colsample_bytree = .5), LDtrain, nround = nround,eval=cidx_lgb_func,
nfold = nfolds, verbose = 0, early_stopping_rounds = early_stopping_rounds)
model
}
lgb_cix_m<-lgb.sur(x_train,y_train,method = 'C')
y_lgcox_pred <- y_lgcidx_pred <- matrix(0, n - tt, 5)
for (i in seq_len(5)) {
y_lgcox_pred[, i] <- predict(lgb_cox_m$boosters[[i]]$booster, x_test)
y_lgcidx_pred[, i] <- predict(lgb_cix_m$boosters[[i]]$booster, x_test)
}
y_lgcox_predict <- -rowMeans(y_lgcox_pred)
y_lgcidx_predict <- -rowMeans(y_lgcidx_pred)
#validation cindex value
cidx_result <- c('XGB_Cox' = concordance(y_test ~ y_xgcox_predict)$con,
'XGB_Cidx' = concordance(y_test ~ y_xgcidx_predict)$con,
'LGB_Cox' = concordance(y_test ~ y_lgcox_predict)$con,
'LGB_Cidx' = concordance(y_test ~ y_lgcidx_predict)$con)
print(cidx_result)
surv_time <- Surv(obs_time, status)
x_test <- X[(tt + 1) : n, ]
y_test <- surv_time[(tt + 1) : n]
cidx_result <- c('XGB_Cox' = concordance(y_test ~ y_xgcox_predict)$con,
'XGB_Cidx' = concordance(y_test ~ y_xgcidx_predict)$con,
'LGB_Cox' = concordance(y_test ~ y_lgcox_predict)$con,
'LGB_Cidx' = concordance(y_test ~ y_lgcidx_predict)$con)
print(cidx_result)
imp_lgb=lgb.importance(lgb_cox_m$boosters[[1]]$booster,percentage = TRUE)
print(imp_lgb)
View(imp_lgb)
plot(imp_lgb)
lgb.plot.importance(imp_lgb,top_n = 10)
xname<-rep(0,100)
for (i in 1:100) {
tx<-paste("X",i,sep='')
xname[i]<-tx
}
colnames(X)<-xname
imp_xgb=xgb.importance(colnames(x_train), model = xgb_cox_m$models[[1]])
imp_xgb_cix=xgb.importance(colnames(x_train), model = xgb_cix_m$models[[1]])
xgb.plot.importance(imp_xgb,top_n = 10)
library(ggplot2)
gg <- xgb.ggplot.importance(imp_xgb, measure = "Frequency", rel_to_first = TRUE))
gg + ggplot2::ylab("Frequency")
(gg <- xgb.ggplot.importance(imp_xgb, measure = "Frequency", rel_to_first = TRUE))
gg + ggplot2::ylab("Frequency")
install.packages("Ckmeans.1d.dp")
library(Ckmeans.1d.dp)
(gg <- xgb.ggplot.importance(imp_xgb, measure = "Frequency", rel_to_first = TRUE))
gg + ggplot2::ylab("Frequency")
(gg <- xgb.ggplot.importance(imp_xgb, top_n = 10,measure = "Frequency", rel_to_first = TRUE))
gg + ggplot2::ylab("Frequency")
devtools::install_github("topycyao/xgblgbcox")
